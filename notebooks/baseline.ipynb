{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model with LightAutoML\n",
    "\n",
    "This notebook demonstrates how to build a baseline model using LightAutoML for viral repository prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightautoml.dataset.roles import ColumnRole, DatetimeRole\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with uv for fast installation\n",
    "!uv pip install pandas scipy numpy bentoml scikit-learn optuna\n",
    "!pip install lightautoml[all]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from lightautoml.dataset.roles import ColumnRole, DatetimeRole\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task\n",
    "from lightautoml.tuner.optuna import OptunaTuner\n",
    "from lightautoml.ml_algo.boost_lgbm import LGBMClassifier\n",
    "from lightautoml.tuner.tuner import AutoTuner\n",
    "from lightautoml.pipelines.selection.base import ModelSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Prepare the dataframe and roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/repos_snapshot.parquet\")\n",
    "\n",
    "TARGET = \"viral_label\"          # 1 / 0\n",
    "# if you predict both labels, train two separate models or a multitask preset\n",
    "\n",
    "# Define numeric features based on RepoData model\n",
    "numeric_features = [\n",
    "    # 30-day metrics\n",
    "    \"stars_30d\", \"forks_30d\", \"commits_30d\", \"contributors_30d\",\n",
    "    # 90-day metrics  \n",
    "    \"stars_90d\", \"forks_90d\", \"commits_90d\", \"contributors_90d\",\n",
    "    # 365-day metrics\n",
    "    \"stars_365d\", \"forks_365d\", \"commits_365d\", \"contributors_365d\",\n",
    "    # Repository metadata\n",
    "    \"repo_age\"\n",
    "]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = [\"primary_language\", \"license\"]\n",
    "\n",
    "# Define boolean features (will be treated as categorical)\n",
    "boolean_features = [\"has_ci\", \"has_wiki\"]\n",
    "\n",
    "roles = {\n",
    "    \"target\": TARGET,\n",
    "    \"drop\": [\"repo_id\", \"abandoned_label\"],  # drop ID and unused label\n",
    "    \"numeric\": numeric_features,\n",
    "    \"categorical\": categorical_features + boolean_features,\n",
    "    \"datetime\": {\"snapshot_date\": DatetimeRole(base_date=True)}\n",
    "}\n",
    "\n",
    "# LightAutoML infers roles automatically, but it's safer to pass them explicitly when your columns are heterogeneous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dataset structure\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumns in dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df[TARGET].value_counts())\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sum())\n",
    "\n",
    "# Check if all expected features are present\n",
    "expected_features = numeric_features + categorical_features + boolean_features + [\"snapshot_date\", TARGET]\n",
    "missing_features = [f for f in expected_features if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\nWarning: Missing expected features: {missing_features}\")\n",
    "else:\n",
    "    print(f\"\\nAll expected features present ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper train/test split for model validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    # Split data before AutoML to have proper holdout validation\n",
    "    train_data, test_data = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=df[TARGET]\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {train_data.shape}\")\n",
    "    print(f\"Test set: {test_data.shape}\")\n",
    "    print(f\"Target distribution in train: {train_data[TARGET].value_counts()}\")\n",
    "    print(f\"Target distribution in test: {test_data[TARGET].value_counts()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in train/test split: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Define the task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task(\n",
    "    name=\"binary\", \n",
    "    metric=\"aucpr\",           # PR-AUC is better for class imbalance\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = TabularAutoML(\n",
    "    task               = task,\n",
    "    timeout            = 3600,          # seconds – wall-clock budget\n",
    "    cpu_limit          = 8,             # threads\n",
    "    reader_params      = {\"n_jobs\": 8}, # FAST reader\n",
    "    verbose            = 1\n",
    ")\n",
    "\n",
    "oof_pred = automl.fit_predict(train_data, roles=roles)\n",
    "\n",
    "print(\"CV PR-AUC:\", automl.score(oof_pred, df[TARGET]))\n",
    "best_pipeline = automl.create_model(\"LightGBM\")[0]\n",
    "\n",
    "# Save the trained model\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the full automl model\n",
    "with open(models_dir / \"baseline_automl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(automl, f)\n",
    "print(\"Saved baseline AutoML model to ../models/baseline_automl.pkl\")\n",
    "\n",
    "# The preset internally builds a 3-layer ensemble (linear + LightGBM + CatBoost) with default HPO based on random search inside each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom hyper-parameter optimisation\n",
    "\n",
    "LightAutoML exposes an \"AutoTune\" wrapper that can replace the default tuning logic with Optuna (or hyperopt, or random).\n",
    "You can create a tuned LightGBM block and inject it into the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣  Define base algorithm\n",
    "lgb_params = {\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 128,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"min_data_in_leaf\": 5,\n",
    "}\n",
    "\n",
    "lgb = LGBMClassifier(**lgb_params, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣  Wrap with Optuna tuner\n",
    "optuna_tuner = OptunaTuner(\n",
    "    search_space={\n",
    "        \"learning_rate\": (0.005, 0.2, \"loguniform\"),\n",
    "        \"num_leaves\": (16, 512, \"int\"),\n",
    "        \"feature_fraction\": (0.5, 1.0, \"uniform\"),\n",
    "        \"min_data_in_leaf\": (1, 50, \"int\"),\n",
    "        \"lambda_l2\": (0.0, 5.0, \"loguniform\"),\n",
    "    },\n",
    "    n_trials=100,\n",
    "    timeout=1800,          # 30 min tuning budget inside global timeout\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "tuned_lgb = AutoTuner(\n",
    "    lgb,\n",
    "    tuner=optuna_tuner,\n",
    "    cv=automl.reader.cv_splitter,   # reuse same CV object\n",
    "    scoring=task.metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣  Plug tuned block into an ensemble\n",
    "custom_selector = ModelSelector(models=[tuned_lgb])\n",
    "\n",
    "automl_custom = TabularAutoML(\n",
    "    task      = task,\n",
    "    timeout   = 5400,\n",
    "    cpu_limit = 8,\n",
    "    general_params = {\"use_algos\": [[custom_selector]]},  # single layer\n",
    "    verbose   = 1\n",
    ")\n",
    "\n",
    "oof_tuned = automl_custom.fit_predict(train_data, roles=roles)\n",
    "print(\"Tuned PR-AUC:\", automl_custom.score(oof_tuned, df[TARGET]))\n",
    "\n",
    "# Save the custom tuned model\n",
    "with open(models_dir / \"tuned_automl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(automl_custom, f)\n",
    "print(\"Saved tuned AutoML model to ../models/tuned_automl.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Validation and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, classification_report\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=== BASELINE MODEL VALIDATION ===\")\n",
    "test_pr_auc_baseline = test_roc_auc_baseline = None\n",
    "if 'automl' in locals():\n",
    "    test_pred_baseline = automl.predict(test_data)\n",
    "    test_pr_auc_baseline = average_precision_score(test_data[TARGET], test_pred_baseline.data[:, 0])\n",
    "    test_roc_auc_baseline = roc_auc_score(test_data[TARGET], test_pred_baseline.data[:, 0])\n",
    "    print(f\"Baseline Test PR-AUC: {test_pr_auc_baseline:.4f}\")\n",
    "    print(f\"Baseline Test ROC-AUC: {test_roc_auc_baseline:.4f}\")\n",
    "\n",
    "print(\"\\n=== TUNED MODEL VALIDATION ===\")\n",
    "test_pr_auc_tuned = test_roc_auc_tuned = None\n",
    "if 'automl_custom' in locals():\n",
    "    test_pred_tuned = automl_custom.predict(test_data)\n",
    "    test_pr_auc_tuned = average_precision_score(test_data[TARGET], test_pred_tuned.data[:, 0])\n",
    "    test_roc_auc_tuned = roc_auc_score(test_data[TARGET], test_pred_tuned.data[:, 0])\n",
    "    print(f\"Tuned Test PR-AUC: {test_pr_auc_tuned:.4f}\")\n",
    "    print(f\"Tuned Test ROC-AUC: {test_roc_auc_tuned:.4f}\")\n",
    "    if test_pr_auc_baseline is not None:\n",
    "        print(f\"\\nImprovement from tuning: {test_pr_auc_tuned - test_pr_auc_baseline:.4f} PR-AUC points\")\n",
    "    best_pred = test_pred_tuned\n",
    "    best_pred_binary = (best_pred.data[:, 0] > 0.5).astype(int)\n",
    "    print(\"\\nClassification Report (threshold=0.5):\")\n",
    "    print(classification_report(test_data[TARGET], best_pred_binary))\n",
    "    try:\n",
    "        best_model = automl_custom\n",
    "        fi = best_model.get_feature_scores()\n",
    "        if fi is not None:\n",
    "            print(\"Top 15 most important features:\")\n",
    "            for i, (feature, importance) in enumerate(fi.head(15).items()):\n",
    "                print(f\"{i+1:2d}. {feature}: {importance:.4f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "performance_summary = {\n",
    "    \"validation_date\": datetime.now().isoformat(),\n",
    "    \"test_set_size\": test_data.shape[0],\n",
    "    \"baseline_model\": {\n",
    "        \"cv_pr_auc\": float(automl.score(oof_pred, train_data[TARGET])) if 'automl' in locals() else None,\n",
    "        \"test_pr_auc\": float(test_pr_auc_baseline) if test_pr_auc_baseline is not None else None,\n",
    "        \"test_roc_auc\": float(test_roc_auc_baseline) if test_roc_auc_baseline is not None else None\n",
    "    },\n",
    "    \"tuned_model\": {\n",
    "        \"cv_pr_auc\": float(automl_custom.score(oof_tuned, train_data[TARGET])) if 'automl_custom' in locals() else None,\n",
    "        \"test_pr_auc\": float(test_pr_auc_tuned) if test_pr_auc_tuned is not None else None,\n",
    "        \"test_roc_auc\": float(test_roc_auc_tuned) if test_roc_auc_tuned is not None else None\n",
    "    }\n",
    "}\n",
    "\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "with open(models_dir / \"model_performance.json\", \"w\") as f:\n",
    "    json.dump(performance_summary, f, indent=2)\n",
    "print(f\"\\nPerformance summary saved to {models_dir / 'model_performance.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
